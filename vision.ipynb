{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vison \n",
    "Here are the core codes and respective explanation of the vison information processing module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining the central coordinates of the target object\n",
    "\n",
    "Image acquisition and preprocessing: Here, RGB and depth images are obtained through ROS, and the format is converted using CvBridge. Camera intrinsic parameters are also retrieved to facilitate subsequent coordinate transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = CvBridge()\n",
    "cv_image = bridge.imgmsg_to_cv2(img, \"bgr8\")\n",
    "depth_image = bridge.imgmsg_to_cv2(depth_img, \"passthrough\")\n",
    "cv2.imwrite(\"save.jpg\", cv_image)\n",
    "\n",
    "fx, fy, cx, cy = camera_info.K[0], camera_info.K[4],\n",
    "camera_info.K[2], camera_info.K[5]\n",
    "camera_matrix = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the color range and apply color filtering: Here, the color range of the target object is defined, and the RGB image is converted to the HSV color space. A mask image is obtained by applying the color filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_ranges = {\n",
    "  \"green\": ([50, 70, 80], [90, 255, 255]),\n",
    "  \"purple\": ([75, 75, 115], [255, 145, 195]),\n",
    "  \"pink\": ([150, 80, 100], [200, 200, 255]),\n",
    "  \"pink2\": ([0, 130, 130], [30, 170, 170]),\n",
    "}\n",
    "\n",
    "hsv_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "mask_all = None\n",
    "filtered_contours = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contour detection and center coordinate calculation: Contour detection is used to obtain the contours of the target object, and the vertices of the contour are obtained through polygon approximation. The contour with the largest area is selected, and its center coordinates are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for color, (lower, upper) in color_ranges.items():\n",
    "    lower = np.array(lower)\n",
    "    upper = np.array(upper)\n",
    "    mask = cv2.inRange(hsv_frame, lower, upper)\n",
    "    num_labels, labels, stats, centroids =\n",
    "cv2.connectedComponentsWithStats(mask)\n",
    "\n",
    "    min_area_threshold = 100\n",
    "    for i in range(1, num_labels):\n",
    "        area = stats[i, cv2.CC_STAT_AREA]\n",
    "        if area < min_area_threshold:\n",
    "            labels[labels == i] = 0\n",
    "\n",
    "    mask = np.where(labels > 0, 255, 0).astype(np.uint8)\n",
    "    if mask_all is None:\n",
    "        mask_all = mask\n",
    "    else:\n",
    "        mask_all = cv2.bitwise_or(mask_all, mask)\n",
    "        \n",
    "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL,\n",
    "cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for contour in contours:\n",
    "    approx = cv2.approxPolyDP(contour, 0.01 *\n",
    "cv2.arcLength(contour, True), True)\n",
    "    if len(approx) <= 8:\n",
    "        filtered_contours.append((contour, color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of the 3D coordinates of the grasping point: By using the pixel coordinates of the objectâ€™s center and combining them with depth image information, the 3D coordinates are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = cv2.moments(max_contour)\n",
    "\n",
    "if M[\"m00\"] != 0:\n",
    "    center_x = int(M[\"m10\"] / M[\"m00\"]) + x1\n",
    "    center_y = int(M[\"m01\"] / M[\"m00\"]) + y1\n",
    "    center = (center_x, center_y)\n",
    "\n",
    "else:\n",
    "    center = None\n",
    "    \n",
    "u, v = center\n",
    "center_3d = contour23D(u, v)\n",
    "center_x, center_y, center_z = center_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of the surface normal based on point cloud information\n",
    "\n",
    "Reading point cloud data and constructing a KD tree: Point cloud data is retrieved from the point cloud message, and a point cloud object is constructed. Downsampling is performed, and a KD tree is built to quickly query neighboring points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.array(list(pc2.read_points(point_cloud, skip_nans=True,\n",
    "field_names=(\"x\", \"y\", \"z\"))))\n",
    "pc = o3d.geometry.PointCloud()\n",
    "pc.points = o3d.utility.Vector3dVector(points)\n",
    "pc = pc.voxel_down_sample(voxel_size=0.003)\n",
    "pc_tree = o3d.geometry.KDTreeFlann(pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining point cloud data near the grasping point: Query the point cloud near the grasping point and create a new point cloud object for the neighboring points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_3d = np.array([center_x, center_y, center_z])\n",
    "radius = 0.03\n",
    "[k, idx, _] = pc_tree.search_radius_vector_3d(center_3d, radius)\n",
    "nearby_points = np.asarray(pc.points)[idx, :] - center_3d\n",
    "nearby_pc = o3d.geometry.PointCloud()\n",
    "nearby_pc.points = o3d.utility.Vector3dVector(nearby_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICP alignment and surface normal calculation: Using the ICP alignment method, the neighborhood point cloud is aligned with the ideal plane, the rotation matrix is obtained, and the surface normal is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_points = [[center_3d[0], center_3d[1], center_3d[2]]]\n",
    "num_random_pnts = 500\n",
    "random_r = radius * np.sqrt(np.random.uniform(0, 1,\n",
    "num_random_pnts))\n",
    "random_theta = np.random.uniform(0, 2 * np.pi, num_random_pnts)\n",
    "for i in range(num_random_pnts):\n",
    "    target_points.append([random_r[i] * np.cos(random_theta[i]),\n",
    "random_r[i] * np.sin(random_theta[i]), 0])\n",
    "\n",
    "target_points = np.array(target_points)\n",
    "target = o3d.geometry.PointCloud()\n",
    "target.points = o3d.utility.Vector3dVector(target_points)\n",
    "\n",
    "threshold = 0.02\n",
    "trans_init = np.identity(4)\n",
    "\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "  nearby_pc, target, threshold, trans_init,\n",
    " o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    " o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=20\n",
    "00) )\n",
    "\n",
    "rotation_matrix = reg_p2p.transformation\n",
    "ideal_normal = np.array([0, 0, 1])\n",
    "normal1 = ideal_normal.dot((rotation_matrix[:3, :3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publishing color and results \n",
    "Based on the color of the block with the largest detected area, the color information is published.\n",
    "\n",
    "The corresponding information is published to the tf node (including the surface normal and coordinates of the grasping point) and the ros topic (including the color of the grasped block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global color_pub\n",
    "if max_color == \"green\":\n",
    "    color_pub.publish(\"green\")\n",
    "elif max_color == \"purple\":\n",
    "    color_pub.publish(\"purple\")\n",
    "elif max_color == \"pink\" or max_color == \"pink2\":\n",
    "color_pub.publish(\"red\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
